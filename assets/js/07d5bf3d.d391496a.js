"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[1664],{8453(e,n,i){i.d(n,{R:()=>a,x:()=>c});var s=i(6540);const t={},o=s.createContext(t);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(o.Provider,{value:n},e.children)}},9937(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"chapter_2_voice_to_action","title":"Voice Command Integration","description":"Using Whisper or Speech Recognition","source":"@site/docs/chapter_2_voice_to_action.md","sourceDirName":".","slug":"/chapter_2_voice_to_action","permalink":"/ai-book/docs/chapter_2_voice_to_action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter_2_voice_to_action.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Vision-Language-Action","permalink":"/ai-book/docs/chapter_1_vla_overview"},"next":{"title":"Cognitive Planning","permalink":"/ai-book/docs/chapter_3_cognitive_planning"}}');var t=i(4848),o=i(8453);const a={},c="Voice Command Integration",r={},l=[{value:"Using Whisper or Speech Recognition",id:"using-whisper-or-speech-recognition",level:2},{value:"How Voice Capture Works",id:"how-voice-capture-works",level:3},{value:"Whisper Capabilities",id:"whisper-capabilities",level:3},{value:"Parsing Commands",id:"parsing-commands",level:2},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Entity Extraction",id:"entity-extraction",level:3},{value:"Context Awareness",id:"context-awareness",level:3},{value:"Handling Ambiguity",id:"handling-ambiguity",level:2},{value:"Clarification Strategies",id:"clarification-strategies",level:3},{value:"Confidence Scoring",id:"confidence-scoring",level:3},{value:"Hands-on: Voice Command Processing",id:"hands-on-voice-command-processing",level:2},{value:"Example Command Flow",id:"example-command-flow",level:3},{value:"Key Takeaway",id:"key-takeaway",level:2},{value:"Summary and Review",id:"summary-and-review",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:3},{value:"Assessment Questions",id:"assessment-questions",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:3}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-command-integration",children:"Voice Command Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"using-whisper-or-speech-recognition",children:"Using Whisper or Speech Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Speech recognition is a critical component of Vision-Language-Action systems, enabling robots to understand human commands through natural language. OpenAI Whisper and similar systems convert spoken language into text that can be processed by language models and planning systems."}),"\n",(0,t.jsx)(n.h3,{id:"how-voice-capture-works",children:"How Voice Capture Works"}),"\n",(0,t.jsx)(n.p,{children:"The process of capturing and processing voice commands involves several steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Input"}),": Microphones capture the spoken command as an audio signal"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Preprocessing"}),": The audio is cleaned and prepared for analysis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Extraction"}),": Key features of the audio signal are identified"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recognition"}),": The system matches audio features to known language patterns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text Output"}),": The recognized speech is converted to text"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whisper-capabilities",children:"Whisper Capabilities"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is particularly well-suited for robotics applications because:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"It supports multiple languages"}),"\n",(0,t.jsx)(n.li,{children:"It handles various accents and speaking styles"}),"\n",(0,t.jsx)(n.li,{children:"It's robust to background noise"}),"\n",(0,t.jsx)(n.li,{children:"It can be run locally without requiring internet access"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"parsing-commands",children:"Parsing Commands"}),"\n",(0,t.jsx)(n.p,{children:"Once speech is converted to text, the next step is to parse the command and understand its intent. This involves:"}),"\n",(0,t.jsx)(n.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,t.jsx)(n.p,{children:"Identifying what the user wants the robot to do:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Navigation commands ("Go to the kitchen")'}),"\n",(0,t.jsx)(n.li,{children:'Manipulation commands ("Pick up the object")'}),"\n",(0,t.jsx)(n.li,{children:'Interaction commands ("Wave to the person")'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"entity-extraction",children:"Entity Extraction"}),"\n",(0,t.jsx)(n.p,{children:"Identifying specific objects, locations, or parameters mentioned in the command:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Objects: "the red cup", "the book", "the blue box"'}),"\n",(0,t.jsx)(n.li,{children:'Locations: "kitchen", "living room", "charging station"'}),"\n",(0,t.jsx)(n.li,{children:'Actions: "pick up", "move to", "open"'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"context-awareness",children:"Context Awareness"}),"\n",(0,t.jsx)(n.p,{children:"Understanding commands in context:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"That one" refers to an object previously mentioned or visible'}),"\n",(0,t.jsx)(n.li,{children:'"Over there" refers to a location in the robot\'s field of view'}),"\n",(0,t.jsx)(n.li,{children:'"Like I showed you" refers to a previously demonstrated action'}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,t.jsx)(n.p,{children:"Natural language often contains ambiguity that needs to be resolved:"}),"\n",(0,t.jsx)(n.h3,{id:"clarification-strategies",children:"Clarification Strategies"}),"\n",(0,t.jsx)(n.p,{children:"When a command is unclear, the robot can:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Ask for clarification: "Which cup did you mean?"'}),"\n",(0,t.jsx)(n.li,{children:'Present options: "Did you mean the red cup or the blue cup?"'}),"\n",(0,t.jsx)(n.li,{children:"Use context: If only one cup is visible, assume that one is meant"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"confidence-scoring",children:"Confidence Scoring"}),"\n",(0,t.jsx)(n.p,{children:"Speech recognition systems provide confidence scores that indicate how certain they are about their transcription. Commands with low confidence might need to be repeated or clarified."}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-voice-command-processing",children:"Hands-on: Voice Command Processing"}),"\n",(0,t.jsx)(n.p,{children:"Let's explore how a voice command flows through the VLA system:"}),"\n",(0,t.jsx)(n.h3,{id:"example-command-flow",children:"Example Command Flow"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'User says: "Robot, please go to the kitchen and bring me a cup"'}),"\n",(0,t.jsx)(n.li,{children:'Speech recognition converts to: "Robot, please go to the kitchen and bring me a cup"'}),"\n",(0,t.jsx)(n.li,{children:"Intent classification identifies: Navigation task + Manipulation task"}),"\n",(0,t.jsx)(n.li,{children:'Entity extraction identifies: Location "kitchen", Object "cup"'}),"\n",(0,t.jsx)(n.li,{children:"Planning system generates: Navigation plan to kitchen \u2192 Object detection \u2192 Grasping action \u2192 Return navigation"}),"\n",(0,t.jsx)(n.li,{children:"Actions are executed via ROS 2 action sequences"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This example demonstrates how voice commands are transformed into executable robot behaviors through the VLA pipeline."}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaway",children:"Key Takeaway"}),"\n",(0,t.jsx)(n.p,{children:"Voice command integration bridges human communication and robot action. By converting speech to text and parsing the meaning, robots can understand and execute complex commands in natural language, making them more accessible and intuitive to use."}),"\n",(0,t.jsx)(n.p,{children:"In the next chapter, we'll explore how cognitive planning translates these parsed commands into sequences of robot actions."}),"\n",(0,t.jsx)(n.h2,{id:"summary-and-review",children:"Summary and Review"}),"\n",(0,t.jsx)(n.h3,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech recognition using Whisper or equivalent systems"}),"\n",(0,t.jsx)(n.li,{children:"Command parsing and intent classification"}),"\n",(0,t.jsx)(n.li,{children:"Entity extraction from natural language"}),"\n",(0,t.jsx)(n.li,{children:"Handling ambiguity in voice commands"}),"\n",(0,t.jsx)(n.li,{children:"Voice-to-action pipeline flow"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"What are the main steps in the voice command processing pipeline?"}),"\n",(0,t.jsx)(n.li,{children:"How does intent classification work in voice command parsing?"}),"\n",(0,t.jsx)(n.li,{children:"What strategies can be used to handle ambiguous commands?"}),"\n",(0,t.jsx)(n.li,{children:"What is the role of confidence scoring in speech recognition?"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issue"}),": Speech recognition returning incorrect text\n",(0,t.jsx)(n.strong,{children:"Solution"}),": Check audio quality, background noise, and ensure the system supports the speaker's accent"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issue"}),": Difficulty with entity extraction\n",(0,t.jsx)(n.strong,{children:"Solution"}),": Use specific, unambiguous language and provide context when possible"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Issue"}),": Commands with low confidence scores\n",(0,t.jsx)(n.strong,{children:"Solution"}),": Speak more clearly, reduce background noise, or repeat the command"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);