"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[9248],{6470(n,e,i){i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter_3_cognitive_planning","title":"Cognitive Planning","description":"LLM-driven Cognitive Planning","source":"@site/docs/chapter_3_cognitive_planning.md","sourceDirName":".","slug":"/chapter_3_cognitive_planning","permalink":"/ai-book/docs/chapter_3_cognitive_planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter_3_cognitive_planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice Command Integration","permalink":"/ai-book/docs/chapter_2_voice_to_action"},"next":{"title":"Capstone: Full Autonomous Task","permalink":"/ai-book/docs/chapter_4_capstone_autonomous_humanoid"}}');var s=i(4848),a=i(8453);const l={},o="Cognitive Planning",r={},c=[{value:"LLM-driven Cognitive Planning",id:"llm-driven-cognitive-planning",level:2},{value:"Planning Process Overview",id:"planning-process-overview",level:3},{value:"Action Sequences",id:"action-sequences",level:3},{value:"Mapping Actions to Perception",id:"mapping-actions-to-perception",level:2},{value:"Perception Integration",id:"perception-integration",level:3},{value:"Real-time Adaptation",id:"real-time-adaptation",level:3},{value:"Error Handling &amp; Recovery",id:"error-handling--recovery",level:2},{value:"Common Failure Modes",id:"common-failure-modes",level:3},{value:"Recovery Strategies",id:"recovery-strategies",level:3},{value:"Planning with Uncertainty",id:"planning-with-uncertainty",level:3},{value:"Hands-on: Planning Pipeline",id:"hands-on-planning-pipeline",level:2},{value:"Example Planning Flow",id:"example-planning-flow",level:3},{value:"Key Takeaway",id:"key-takeaway",level:2},{value:"Summary and Review",id:"summary-and-review",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:3},{value:"Assessment Questions",id:"assessment-questions",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:3}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"cognitive-planning",children:"Cognitive Planning"})}),"\n",(0,s.jsx)(e.h2,{id:"llm-driven-cognitive-planning",children:"LLM-driven Cognitive Planning"}),"\n",(0,s.jsx)(e.p,{children:"Cognitive planning is the brain of the Vision-Language-Action system, responsible for translating high-level commands into sequences of executable actions. Large Language Models (LLMs) serve as the cognitive engine that performs this translation by understanding the context, goals, and constraints of a given task."}),"\n",(0,s.jsx)(e.h3,{id:"planning-process-overview",children:"Planning Process Overview"}),"\n",(0,s.jsx)(e.p,{children:"The cognitive planning process involves several key steps:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Command Interpretation"}),": The LLM analyzes the parsed command to understand the desired outcome"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Context Analysis"}),": The system considers the current robot state, environment, and available capabilities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Plan Generation"}),": A sequence of actions is generated to achieve the goal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Validation"}),": The plan is checked for feasibility and safety"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Execution"}),": The plan is sent to the robot's action execution system"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"action-sequences",children:"Action Sequences"}),"\n",(0,s.jsx)(e.p,{children:'LLMs can generate action sequences by breaking down complex commands into simpler steps. For example, the command "Go to the kitchen and bring me a cup" might be translated to:'}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Plan navigation to kitchen location"}),"\n",(0,s.jsx)(e.li,{children:"Execute navigation action"}),"\n",(0,s.jsx)(e.li,{children:"Detect cup object in environment"}),"\n",(0,s.jsx)(e.li,{children:"Plan manipulation to grasp cup"}),"\n",(0,s.jsx)(e.li,{children:"Execute grasping action"}),"\n",(0,s.jsx)(e.li,{children:"Plan navigation back to user"}),"\n",(0,s.jsx)(e.li,{children:"Execute return navigation"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"mapping-actions-to-perception",children:"Mapping Actions to Perception"}),"\n",(0,s.jsx)(e.p,{children:"One of the critical aspects of cognitive planning is connecting the planned actions to real-time perception data from the robot's sensors:"}),"\n",(0,s.jsx)(e.h3,{id:"perception-integration",children:"Perception Integration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Detection"}),": Using vision data to locate objects mentioned in commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environment Mapping"}),": Understanding the spatial layout for navigation planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"State Monitoring"}),": Tracking the robot's current state during plan execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feedback Loops"}),": Adjusting plans based on perceived changes in the environment"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"real-time-adaptation",children:"Real-time Adaptation"}),"\n",(0,s.jsx)(e.p,{children:"The planning system must be able to adapt to changes in perception data:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"If an expected object is not found, the plan might need to be revised"}),"\n",(0,s.jsx)(e.li,{children:"If obstacles appear during navigation, the path must be replanned"}),"\n",(0,s.jsx)(e.li,{children:"If a manipulation attempt fails, alternative strategies must be considered"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"error-handling--recovery",children:"Error Handling & Recovery"}),"\n",(0,s.jsx)(e.p,{children:"Even well-planned actions can fail due to various factors in real-world environments. Robust cognitive planning includes strategies for handling these failures:"}),"\n",(0,s.jsx)(e.h3,{id:"common-failure-modes",children:"Common Failure Modes"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Navigation Failures"}),": Robot unable to reach intended destination"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Manipulation Failures"}),": Unable to grasp or manipulate objects as planned"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Perception Failures"}),": Inability to detect required objects or features"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Communication Failures"}),": Issues with ROS 2 action execution"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"recovery-strategies",children:"Recovery Strategies"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Retry with Variation"}),": Attempt the same action with slightly different parameters"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Alternative Approaches"}),": Use different methods to achieve the same goal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Request Assistance"}),": Ask for human intervention when autonomy fails"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safe State"}),": Return to a safe configuration when errors occur"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"planning-with-uncertainty",children:"Planning with Uncertainty"}),"\n",(0,s.jsx)(e.p,{children:"Cognitive planning systems should account for uncertainty in both perception and action execution:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Include confidence measures in plan steps"}),"\n",(0,s.jsx)(e.li,{children:"Plan alternative paths in case of failure"}),"\n",(0,s.jsx)(e.li,{children:"Use probabilistic models to predict action outcomes"}),"\n",(0,s.jsx)(e.li,{children:"Continuously update plans based on new information"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-planning-pipeline",children:"Hands-on: Planning Pipeline"}),"\n",(0,s.jsx)(e.p,{children:"Let's examine a complete planning pipeline example:"}),"\n",(0,s.jsx)(e.h3,{id:"example-planning-flow",children:"Example Planning Flow"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Input"}),': User command "Find the red ball and bring it to me"']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LLM Processing"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Interprets command as object search + manipulation task"}),"\n",(0,s.jsx)(e.li,{children:'Identifies "red ball" as target object and "to me" as destination'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Context Integration"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Current robot position: living room"}),"\n",(0,s.jsx)(e.li,{children:"Known locations: kitchen, bedroom, office"}),"\n",(0,s.jsx)(e.li,{children:"Available actions: navigation, object detection, grasping"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Plan Generation"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Search for red ball in likely locations"}),"\n",(0,s.jsx)(e.li,{children:"Navigate to ball location"}),"\n",(0,s.jsx)(e.li,{children:"Detect and confirm red ball"}),"\n",(0,s.jsx)(e.li,{children:"Grasp the ball"}),"\n",(0,s.jsx)(e.li,{children:"Navigate back to user"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Execution Monitoring"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Each step monitored for success/failure"}),"\n",(0,s.jsx)(e.li,{children:"Plan adjusted if obstacles or issues arise"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"This example demonstrates how cognitive planning bridges the gap between high-level language commands and low-level robot actions."}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaway",children:"Key Takeaway"}),"\n",(0,s.jsx)(e.p,{children:"Cognitive planning is the critical component that transforms language understanding into physical action. By leveraging LLMs for planning and integrating perception data, robots can execute complex tasks while adapting to the uncertainties of real-world environments. This capability is essential for making VLA systems robust and effective."}),"\n",(0,s.jsx)(e.p,{children:"In the next chapter, we'll explore the complete VLA pipeline in a capstone project."}),"\n",(0,s.jsx)(e.h2,{id:"summary-and-review",children:"Summary and Review"}),"\n",(0,s.jsx)(e.h3,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"LLM-driven cognitive planning process"}),"\n",(0,s.jsx)(e.li,{children:"Mapping actions to perception data"}),"\n",(0,s.jsx)(e.li,{children:"Error handling and recovery strategies"}),"\n",(0,s.jsx)(e.li,{children:"Planning with uncertainty"}),"\n",(0,s.jsx)(e.li,{children:"Integration of perception and action"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"What are the key steps in the cognitive planning process?"}),"\n",(0,s.jsx)(e.li,{children:"How does perception data influence action planning?"}),"\n",(0,s.jsx)(e.li,{children:"What are common failure modes in cognitive planning?"}),"\n",(0,s.jsx)(e.li,{children:"How can planning systems handle uncertainty in real-world environments?"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Issue"}),": Plans failing due to environmental changes\n",(0,s.jsx)(e.strong,{children:"Solution"}),": Implement real-time plan monitoring and adjustment capabilities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Issue"}),": Difficulty with multi-step task planning\n",(0,s.jsx)(e.strong,{children:"Solution"}),": Break complex tasks into simpler, sequential subtasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Issue"}),": Perception-action mismatches\n",(0,s.jsx)(e.strong,{children:"Solution"}),": Ensure tight integration between perception and planning components"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>l,x:()=>o});var t=i(6540);const s={},a=t.createContext(s);function l(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);