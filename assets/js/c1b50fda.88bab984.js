"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4911],{8453(e,n,t){t.d(n,{R:()=>s,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},9586(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"reference_modules_integration","title":"Module Integration Reference","description":"This document provides reference materials for how the Vision-Language-Action (VLA) module integrates with previous modules.","source":"@site/docs/reference_modules_integration.md","sourceDirName":".","slug":"/reference_modules_integration","permalink":"/docs/reference_modules_integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/reference_modules_integration.md","tags":[],"version":"current","frontMatter":{}}');var o=t(4848),r=t(8453);const s={},a="Module Integration Reference",l={},c=[{value:"Integration with Module 1: The Robotic Nervous System (ROS 2)",id:"integration-with-module-1-the-robotic-nervous-system-ros-2",level:2},{value:"Integration with Module 2: The Digital Twin (Gazebo &amp; Unity)",id:"integration-with-module-2-the-digital-twin-gazebo--unity",level:2},{value:"Integration with Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",id:"integration-with-module-3-the-ai-robot-brain-nvidia-isaac",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-integration-reference",children:"Module Integration Reference"})}),"\n",(0,o.jsx)(n.p,{children:"This document provides reference materials for how the Vision-Language-Action (VLA) module integrates with previous modules."}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-module-1-the-robotic-nervous-system-ros-2",children:"Integration with Module 1: The Robotic Nervous System (ROS 2)"}),"\n",(0,o.jsx)(n.p,{children:"The VLA module builds on ROS 2 concepts for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Robot communication and control"}),"\n",(0,o.jsx)(n.li,{children:"Action execution through ROS 2 action sequences"}),"\n",(0,o.jsx)(n.li,{children:"Message passing between VLA components"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-module-2-the-digital-twin-gazebo--unity",children:"Integration with Module 2: The Digital Twin (Gazebo & Unity)"}),"\n",(0,o.jsx)(n.p,{children:"The VLA module leverages digital twin concepts for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Simulation environments to test VLA behaviors"}),"\n",(0,o.jsx)(n.li,{children:"Sensor simulation for vision components"}),"\n",(0,o.jsx)(n.li,{children:"Safe testing of voice-commanded robot actions"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-module-3-the-ai-robot-brain-nvidia-isaac",children:"Integration with Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"}),"\n",(0,o.jsx)(n.p,{children:"The VLA module incorporates AI-Robot Brain concepts for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Perception-action loops"}),"\n",(0,o.jsx)(n.li,{children:"Cognitive planning using AI systems"}),"\n",(0,o.jsx)(n.li,{children:"Navigation and path planning integration"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);